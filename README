nml -- Nick's ML

Resurrected from >10 years of slumber:
- Fixed runtime system to work for 64 bits.
- Begin cleanup/reorganization of code (move various things to: to-be-sorted)
- Fixed nml compiler (C-code extractor) to be independent of ML func-arg evaluation order

The nml system is not quite a REPL. But a slightly more primitive beast which is directed by command line arguments to:
(1) load an ML file
(2) Execute a ML expression
(3) Export an ML value representing a "main" program (of type: string list -> unit) to a file

This explains the name "nux" which might mean: `Nml,Use/eXport'. (But this is useful enough!)


Nux can be used like a top level:
$ boot/nux.exe -x 'fun sqr x = x * x' -x 'sqr 7'
$ boot/nux.exe 'predefined/nml_NonPrim.ML' -x 'open NonPrim; length (explode "something")'

Or like a compiler:
$ boot/nux.exe -x 'fn _ => print "Hello Nux!\n";' --export hello.C

The generated C++ being compiled and linked thus:
$ g++ -Wno-write-strings -Iruntime -c hello.C -o hello.o
$ g++ runtime/nml_runtime.o hello.o -o hello.exe


The nml system in written in ML; the code is in ML/*.ML
Nml is bootstrapped using sml/nj; and can then self-compile. The Makefile defines the phases:

    sml/nj + ML/*.ML compiling ML/*.ML --> boot/nux.C --> boot/nux.exe
    boot/nux.exe compiling ML/*.ML     --> gen1/nux.C --> gen1/nux.exe

(boot/nux.C is provided in case sml/nj is not available)


$ diff {boot,gen1}/nux.C
< Nword n2876_scon = g_mkString ("NML-boot: ");
---
> Nword n2876_scon = g_mkString ("NML-gen1: ");


Profiling and Optimization
------------------------------
The C code extacted by Nml is not yet very quick. (In the following "C" means "C++")

I've decided to use my bedlam solver as the driving example to improve performance. The algorithm is a backtracking search algorithm which finds solutions to the bedlam cube. It just happens that the first solution is found rather quickly - after just 6025 positions are tried -- whereas the 2nd are 3rd solution come much later: 749663, 780903. I've setup the code to stop when the first solution is found. This takes ~1s when running with nj, but 66s for nml :(

It seems with such shocking performance, there is probably some low hanging fruit to discover. I have lots of ideas where the low hanging fruit might be, but I'm very keen to drive the optimization by measurement. initially I will use gprof to collect profiling data from the bedlam run, and see where it points!

I expect some performace gains can be achieved simply by optimizing the runtime system. Others may require changes to the compiler; possibly in conjuction with changes to the runtime system.  I will record here what I changed & the effect on the running time of the bedlam example:

I probably should be a little more rigorous with my measurement; making multiple runs etc. But I find the timings so far as pretty consistent, and if I see a variation I just record the shortest time witnessed.


    66s     initial
    37s     -O
    28.5    -O2
    29.8    -O3 -- worse!
    27.5    -O2, runtime & generated code compiled as a single program unit; (again -O3 is worse)
    24.8    disable asserts
    21.1    remove dynamic_cast for Value_Con0 (boxed arity-0 constructor) in "getTag", which is never constructed
    19.8    -03 (why does -O3 help now but not before?)


Discussion:

Initially some very obvious things had big wins. i.e. gcc compilation optimization - who'd have thought it :)

Compiling as a single program unit was not so dramatic, but it does ensure no inlining opportunities are missed.
Interestingly it seems to speed up compilation of the C code.

It is odd that -O3 is not always better than -O2.

Profile shows that the next thing to attack is the use of dynamic_cast!


Round #2...

    19.8    current state
     6.2    dynamic_cast --> reinterpret_cast (when TYPE_ERROR otherwise)
     4.1    remove if-test when using reinterpret_cast for down_cast
     3.1    inline core functions (-Winline --param inline-unit-growth=100)
